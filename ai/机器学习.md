###  概述
```
    三要素:  数据 算法 算力



    术语:
        样本:       数据集中的一条独立记录或一个观察对象，它是我们分析的基本单位.
        特征:       描述样本的属性、变量或指标。它们是模型的输入，是做出判断的依据.
        标签:       我们想要预测的目标值或分类结果。它是模型的输出，是样本的“答案”.
        训练集:     用于训练模型、让模型学习规律的那部分数据。模型会反复查看训练集，调整内部参数.
        测试集:     用于评估模型最终表现的那部分数据。这部分数据在训练过程中完全不可见，用来检验模型面对新样本时的泛化能力.

    学习方式：
        基于规则的学习: if else
        基于模型的学习:从数据中自动学规律


    算法分类:
        有监督学习: 输入数据 由输入特征值 和 目标值 组成. 输入的训练集有标签.
            分类问题: 目标(标签)值 不连续.  分类种类: 二分类,多分类.
            回归问题: 目标(标签)值 连续.  一元线性回归 y= wx + b

        无监督学习: 输入数据 没有标签.  根据样本间相似性,对样本集聚类.
    
        半监督学习: 有特征,部分有标签.  (可以大幅降低标记成本)
            原理: 
                1.让专家少量标记数据(标签),利用已经标记的数据训练一个模型. 2.利用模型套用未标记的数据(预测).
                2.询问领域专家分类结果与模型分类结果做对比(专家,校验,审核).对模型进一步改善.

        强化学习: 寻找最短路径(最优解),获取最多的奖励.


    建模流程:   
        1.获取数据          
                获取经验数据集.
        2.数据基本处理      
                数据缺失值处理.异常值处理.
        3.特征工程  对数据特征 提取,转成向量.     
            特征工程概念:
                利用专业背景知识和技巧处理数据,让机器学习算法效果最好.这个过程叫特性工程.
                数据 特性 决定机器学习上限.   模型和算法逼近 上限.
            内容:
                特性提取.       -> 原始数据中提取与任务相关的特征.构成特征向量.
                特征预处理.     -> 因量纲问题,有的特征对模型影响大,有的特征对模型影响小. 不同单位的特征数据转换为同一个范围.
                    归一化.  x' =  (当前值 - 最小值) / (最大值 - 最小值) 
                    标准化.
                特征降维.       -> 原始数据维度降低.  (3维->2维 改变原数据)
                特征选取.       -> 从原数据中选取和任务相关的特征集合子集.
                特征组合.       -> 把多个特征合并成一个特征. 乘法或加法来做.
        4.模型训练  选合适的算法对模型进行训练
                线性回归,逻辑回归,决策树,GBDT.
        5.模型评估
                回归评测指标.
                分类评测指标.
                聚类评测指标.


    模型拟合.
        拟合 = 模型在 训练集 和 测试集 上表现情况.
        欠拟合 = 模型在 训练集 和 测试集 表现都很差.
        过拟合 = 模型在 训练集 表现好, 在 测试集 表现差. 
        正好拟合 = 模型在 训练集 和 测试集 表现都好.

        欠拟合原因: 模型过于简单. (特征少)
        过拟合原因: 模型过于复杂. 数据不纯. 训练数据太少.
        泛化: 模型在新数据集(非训练数据集)的表现好坏.
        奥卡姆剃刀原则: 两个具有相同泛化误差的模型,较简单的模型比较复杂的模型更可取.


    机器学习三大问题:
        分类 类聚 回归
``` 


#### 安装 Anaconda

```
    Anaconda prompt:
        pip install scikit-learn   机器学习开源库
```

#### KNN(k近邻) 算法
```
    1.思想
        如果一个样本在特征空间中的k个最相似的样本中的大多数属于某一个类别,则该样本也属于这个类别.
            样本之间的相似性怎么确定?   欧式距离=对应维度差值平方和,开平方根   开根号(平方(x1-x2) + 平台(y1-y2))  



    流程:
        1.计算未知样本到每一个训练样本的距离.
        2.将训练样本按距离大小升序排列.
        3.取出距离最近的k个样本.
    2.分类流程:
        4.多数表决,统计k个样本中哪个类别的样本最多.
        5.将未知样本归类到出现次数最多的类别.
    3.回归流程:
        4.把k个样本的目标值计算 平均值
        5.将未知的样本预测值.


    距离度量单位:
        欧式距离, 两个点在空间中的距离.
        曼哈顿距离, 对应维度差值的绝对值,求和
        切比雪夫距离, 对应维度差值的绝对值,求最大值   国际象棋两个点的距离, 国王可以八个方向走
        闵可夫斯基距离,闵氏距离 对多个距离度量公式的概括表述.


    交叉验证:
    数据集分割方法, 将训练集划分为N份,拿一份做测试集,其他n-1份做训练集. 为了得到更加可信的模型评分.

    网格搜索:
    模型有很多超参,需要手动产生很多超参组合,训练模型.每组超参都用交叉验证评估,最后选出最优参数组合建立模型.
```


#### 特征预处理
```
    归一化:
        通过公式把各列的值映射到默认[0,1]区间 
        公式:
            x' = (当前值 - 该列最小值) / (该列最大值 - 该列最小值)
            x'' = x' * (mx - mi) + mi 

        x'  -->  基于公式算出的结果
        x'' -->  最终结果.
        mx  -->  区间最大值.
        mi  -->  区间最小值.
    弊端: 容易受到最大值和最小值影响, 适合小数据集.


    标准差概念:
        先计算每个数值与平均数的差，然后求其平方值，再把所有平方值相加后除以总数，最后再对结果进行平方根运算.

    标准化:
        转换为均值为0 标准差为1的 标准 正态分布数据.
        x' = (x - mean(特征平均值) ) / σ(标准差)
```

#### 线性回归
```
    定义:
    线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法.
        回归的现代解释：
            回归分析是研究某一变量（因变量）与另一个或多个变量（解释变量、自变量）之间的依存关系，用解释变量的已知值或固定值来估计或预测因变量的总体平均值

    
    一元线性回归:
        
    分类:

    应用场景:


```