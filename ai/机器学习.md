###  概述
```
    三要素:  数据 算法 算力



    术语:
        样本:       数据集中的一条独立记录或一个观察对象，它是我们分析的基本单位.
        特征:       描述样本的属性、变量或指标。它们是模型的输入，是做出判断的依据.
        标签:       我们想要预测的目标值或分类结果。它是模型的输出，是样本的“答案”.
        训练集:     用于训练模型、让模型学习规律的那部分数据。模型会反复查看训练集，调整内部参数.
        测试集:     用于评估模型最终表现的那部分数据。这部分数据在训练过程中完全不可见，用来检验模型面对新样本时的泛化能力.

    学习方式：
        基于规则的学习: if else
        基于模型的学习:从数据中自动学规律


    算法分类:
        有监督学习: 输入数据 由输入特征值 和 目标值 组成. 输入的训练集有标签.
            分类问题: 目标(标签)值 不连续.  分类种类: 二分类,多分类.
            回归问题: 目标(标签)值 连续.  一元线性回归 y= wx + b

        无监督学习: 输入数据 没有标签.  根据样本间相似性,对样本集聚类.
    
        半监督学习: 有特征,部分有标签.  (可以大幅降低标记成本)
            原理: 
                1.让专家少量标记数据(标签),利用已经标记的数据训练一个模型. 2.利用模型套用未标记的数据(预测).
                2.询问领域专家分类结果与模型分类结果做对比(专家,校验,审核).对模型进一步改善.

        强化学习: 寻找最短路径(最优解),获取最多的奖励.


    建模流程:   
        1.获取数据          
                获取经验数据集.
        2.数据基本处理      
                数据缺失值处理.异常值处理.
        3.特征工程  对数据特征 提取,转成向量.     
            特征工程概念:
                利用专业背景知识和技巧处理数据,让机器学习算法效果最好.这个过程叫特性工程.
                数据 特性 决定机器学习上限.   模型和算法逼近 上限.
            内容:
                特性提取.       -> 原始数据中提取与任务相关的特征.构成特征向量.
                特征预处理.     -> 因量纲问题,有的特征对模型影响大,有的特征对模型影响小. 不同单位的特征数据转换为同一个范围.
                    归一化.  x' =  (当前值 - 最小值) / (最大值 - 最小值) 
                    标准化.
                特征降维.       -> 原始数据维度降低.  (3维->2维 改变原数据)
                特征选取.       -> 从原数据中选取和任务相关的特征集合子集.
                特征组合.       -> 把多个特征合并成一个特征. 乘法或加法来做.
        4.模型训练  选合适的算法对模型进行训练
                线性回归,逻辑回归,决策树,GBDT.
        5.模型评估
                回归评测指标.
                分类评测指标.
                聚类评测指标.


    模型拟合.
        拟合 = 模型在 训练集 和 测试集 上表现情况.
        欠拟合 = 模型在 训练集 和 测试集 表现都很差.
        过拟合 = 模型在 训练集 表现好, 在 测试集 表现差. 
        正好拟合 = 模型在 训练集 和 测试集 表现都好.

        欠拟合原因: 模型过于简单. (特征少)
        过拟合原因: 模型过于复杂. 数据不纯. 训练数据太少.
        泛化: 模型在新数据集(非训练数据集)的表现好坏.
        奥卡姆剃刀原则: 两个具有相同泛化误差的模型,较简单的模型比较复杂的模型更可取.


    机器学习三大问题:
        分类 类聚 回归
``` 


#### 安装 Anaconda

```
    Anaconda prompt:
        pip install scikit-learn   机器学习开源库
```

#### KNN(k近邻) 算法
```
    1.思想
        如果一个样本在特征空间中的k个最相似的样本中的大多数属于某一个类别,则该样本也属于这个类别.
            样本之间的相似性怎么确定?   欧式距离=对应维度差值平方和,开平方根   开根号(平方(x1-x2) + 平台(y1-y2))  



    流程:
        1.计算未知样本到每一个训练样本的距离.
        2.将训练样本按距离大小升序排列.
        3.取出距离最近的k个样本.
    2.分类流程:
        4.多数表决,统计k个样本中哪个类别的样本最多.
        5.将未知样本归类到出现次数最多的类别.
    3.回归流程:
        4.把k个样本的目标值计算 平均值
        5.将未知的样本预测值.


    距离度量单位:
        欧式距离, 两个点在空间中的距离.
        曼哈顿距离, 对应维度差值的绝对值,求和
        切比雪夫距离, 对应维度差值的绝对值,求最大值   国际象棋两个点的距离, 国王可以八个方向走
        闵可夫斯基距离,闵氏距离 对多个距离度量公式的概括表述.


    交叉验证:
    数据集分割方法, 将训练集划分为N份,拿一份做测试集,其他n-1份做训练集. 为了得到更加可信的模型评分.

    网格搜索:
    模型有很多超参,需要手动产生很多超参组合,训练模型.每组超参都用交叉验证评估,最后选出最优参数组合建立模型.
```


#### 特征预处理
```
    归一化:
        通过公式把各列的值映射到默认[0,1]区间 
        公式:
            x' = (当前值 - 该列最小值) / (该列最大值 - 该列最小值)
            x'' = x' * (mx - mi) + mi 

        x'  -->  基于公式算出的结果
        x'' -->  最终结果.
        mx  -->  区间最大值.
        mi  -->  区间最小值.
    弊端: 容易受到最大值和最小值影响, 适合小数据集.


    标准差概念:
        先计算每个数值与平均数的差，然后求其平方值，再把所有平方值相加后除以总数，最后再对结果进行平方根运算.

    标准化:
        转换为均值为0 标准差为1的 标准 正态分布数据.
        x' = (x - mean(特征平均值) ) / σ(标准差)
```

#### 线性回归
```
    目的:
        利用回归方程对 一个或多个自变量(特征值)和因变量(目标值)之间 关系进行建模的一种分析方式.
        线性回归属于: 有监督学习, 有特征,有标签, 标签是连续.
    
    数学定义:
    线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法.
        回归的现代解释：
            回归分析是研究某一变量（因变量）与另一个或多个变量（解释变量、自变量）之间的依存关系，用解释变量的已知值或固定值来估计或预测因变量的总体平均值

    分类:
        一元线性回归: 1个特征列 + 1个标签列
        多元线性回归: 多个特征列 + 1个标签列

    公式:
        一元线性回归:
            y = kx + b = wx + b
            k: 数学中斜率, 机器学习中weight(权重), 简称: w
            b: 数学中截距, 机器学习中Bias(偏置), 简称: b
    多远线性回归:
        y = w的转置 * x + b

```

#### 损失函数
```
    误差概念: 预测值y - 真实值y

    损失函数: 衡量每个样本预测值与真实值效果的函数, 也叫代价函数、成本函数、目标函数.

    回归问题中损失函数:
        最小二乘发: 误差平方和
        均方误差:  最小二乘/样本个数
        平均绝对误差: 所有样本绝对值和的平均值
```

#### 基础数学
```
    标量 scalar: 一个独立存在的数,只有大小没有方向.
    向量 vector: 向量指一顺序排列的元素. 默认是列向量.
    矩阵 matrix: 二维数组.
    张量 Tensor: 数组, 张量是基于向量和矩阵的推广.

    导数:
        
    偏导数:
```


#### 逻辑回归
```
逻辑回归（Logistic Regression）是一种广泛应用于分类问题的统计学习方法，尽管名字中带有"回归"，但它实际上是一种用于二分类或多分类问题的算法。

什么是偏置?
固有的、系统的认知偏差.  模型由于其自身假设和简化而导致的系统性预测误差
```

#### 决策树
```
决策树（Decision Tree）是一种常用的机器学习算法，广泛应用于分类和回归问题。

决策树通过树状结构来表示决策过程，每个内部节点代表一个特征或属性的测试，每个分支代表测试的结果，每个叶节点代表一个类别或值。
```

#### 强化学习
```
智能体通过与环境进行马尔可夫决策过程式的交互，根据获得的奖励信号，不断优化其策略（通常通过学习和更新价值函数来实现），最终目标是最大化长期累积奖励
```

#### 深度学习
```
    神经网络:
        模仿人脑神经元的工作方式，通过层层连接与计算，赋予了机器学习和认知的能力.

        神经元	    ---      计算单元，完成加权求和 -> 加偏置 -> 激活函数。
        权重与偏置	---      模型需要学习的核心参数，决定了网络的行为。
        激活函数	---      引入非线性（如ReLU, Sigmoid），使网络能学习复杂关系。
        网络层级	---      输入层（接收数据）、隐藏层（特征提取）、输出层（产生预测）。
        前向传播	---      数据从输入层流向输出层，计算预测值的过程。

        前向传播与反向传播:
            前向传播：数据从输入层，经过隐藏层，最终到达输出层，并产生预测结果的过程。这是一个推理过程。
            反向传播：根据前向传播产生的预测结果与真实值之间的误差，从输出层开始，
                     反向逐层计算每个参数（权重和偏置）对总误差的"贡献"大小（即梯度），并据此更新参数。这是一个学习过程。
```

#### PyTorch 
```
    张量 Tensor 
        PyTorch 中的核心数据结构，用于存储和操作多维数组

        维度（Dimensionality）：张量的维度指的是数据的多维数组结构。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。

        形状（Shape）：张量的形状是指每个维度上的大小。例如，一个形状为(3, 4)的张量意味着它有3行4列。

        数据类型（Dtype）：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如torch.int8、torch.int32）、浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）

    前馈神经网络（Feedforward Neural Networks）：数据单向流动，从输入层到输出层，无反馈连接。
    卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像处理，使用卷积层提取空间特征。
        残差网络 （Residual Network, ResNet）:
            ResNet 是一种通过引入残差学习和跳跃连接的深度卷积神经网络架构，解决了深度神经网络中的梯度消失问题，使得非常深的神经网络成为可能。
            ResNet 的设计理念深刻影响了深度学习的发展，并成为许多计算机视觉任务中广泛使用的基础模型之一
    
    RNN(循环神经网络	Recurrent Neural Network)
        专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系。
        RNN 的特别之处在于它具有"记忆能力"，可以在网络的隐藏状态中保存之前时间步的信息。
        循环神经网络用于处理随时间变化的数据模式。
        在 RNN 中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数


    激活函数（Activation Function）
        激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：
        Sigmoid：用于二分类问题，输出值在 0 和 1 之间。
        Tanh：输出值在 -1 和 1 之间，常用于输出层之前。
        ReLU（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 f(x) = max(0, x)，有助于解决梯度消失问题。
        Softmax：常用于多分类问题的输出层，将输出转换为概率分布。

    损失函数（Loss Function）
        损失函数用于衡量模型的预测值与真实值之间的差异

        均方误差（MSELoss）：回归问题常用，计算输出与目标值的平方差。
        交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵。
        BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。

    优化器（Optimizer）
        优化器负责在训练过程中更新网络的权重和偏置

            SGD（随机梯度下降）
            Adam（自适应矩估计）
            RMSprop（均方根传播）
```

#### Transformer
```
    一种基于自注意力机制（Self-Attention） 的深度学习架构.
    Transformer 的核心思想是完全摒弃传统的循环神经网络（RNN）结构，仅依赖注意力机制来处理序列数据，从而实现更高的并行性和更快的训练速度.
```

图像识别
NLP
RAG

